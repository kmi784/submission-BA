\label{cha:methods}


\section{Monte Carlo simulations}
\label{sec:MCsimulations}

In a statistical physic setup, simulation methods become important either if the enumeration of the microstates reaches a huge number of terms or if there is no 
analytical method to determine the density of states or partition function of the considered system. To overcome these problems, 
sample the time evolution of the system  with fixed macroscopic state variables, i.e., a fixed environment temperature $T$ for the canonical ensemble. 
Hence, the microstates arising should be distributed according to the probability distribution of the canonical ensemble, Equation~\eqref{align:canonical_distribution}, 
(if the system is in equilibrium) and one estimates the expectation value through the given microstates $\bm{\sigma}_k$ where $k$ indexes the $M$ 
measurements. Therefore, an arbitrary observable $\mathcal{O}$ can be estimated by
\begin{align}
    \hat{\mathcal{O}}_T=\frac{1}{M}\sum_{k=1}^M\mathcal{O}(\bm{\sigma}_k)
    \label{align:MCidea}
\end{align}
if $M$ is sufficiently large and the system is properly equilibrated, i.e., the occurrence of the microstates follows the underlying probability 
distribution. To achieve such time evolution of the microstates one can use Markow chain Monte Carlo methods (MCMC methods) originating from the 
work of Metropolis et al.~\cite{Metropolis1953}.

\subsection*{MCMC method}

The idea of MCMC methods is to let the system transit from a microstate $\bm{\sigma}_i$ to $\bm{\sigma}_j$ with a transition probability $W_{ij}$, and 
thus the transition $\bm{\sigma}_i\to\bm{\sigma}_j$ happens with a probability $W_{ij}$ where the case $i\!=\!j$ is allowed as well. To sample the 
underlying distribution, the following requirements for $W_{ij}$ are necessary
\begin{align}
    &W_{ij} \ge 0                        &\forall i,j  
        \label{align:con1}\\
    &\sum_jW_{ij} = 1                    &\forall i
        \label{align:con2}\\
    &W_{ij}P(\bm{\sigma}_i)=W_{ji}P(\bm{\sigma}_j) &\forall i,j
        \label{align:con3}\\
    &\exists\bm{\sigma}_{k_1},...,\bm{\sigma}_{k_n} \  n\!\in\!\mathbb{N} \  \text{  such that} 
                             \sum_{k_1,...,k_n}\!\!\!\!W_{ik_1}\!\!\cdot...\cdot\!W_{k_nj}>0 &\forall i,j.
        \label{align:con4}
\end{align}
Conditions~\eqref{align:con1} and~\eqref{align:con2} ensure the strict positivity and normalization of probabilities. Equation~\eqref{align:con3} is also known as 
detailed balance, implying that a transition $\bm{\sigma}_i\to\bm{\sigma}_j$ is reversible, and hence no directions of the transitions $\bm{\sigma}_i\leftrightarrow\bm{\sigma}_j$ 
is preferred. The balance condition $\sum_iW_{ij}P(\bm{\sigma}_i)=P(\bm{\sigma}_j)$ can easily be obtained by a summation of the detailed balance from which
it follows that the equilibrium probabilities $P(\bm{\sigma}_j)$ are the components of an eigenvector of the transition matrix ($W_{ij}$ can be 
considered as a matrix).
Therefore, the transitions conserve the underlying equilibrium distribution, and it is urgently necessary to equilibrate the system such that the occurrence
of the microstates reflects the equilibrium distribution. Condition~\eqref{align:con4} ensures the ergodicity, i.e., all possible states
can be reached through a finite number of transitions with non-vanishing probability. Thus, no transition traps the system in a sub-state space.~\cite{Janke2012}

In this thesis, all microstate transitions are constructed via the following steps. Firstly, one selects a transition
proposal from $\bm{\sigma}_i$ to $\bm{\sigma}_j$ with probability $f_{ij}$. Secondly, the transition is accepted with a probability $w_{ij}$. Hence, for $i\!\neq\!j$,
the transition probability is  given by $W_{ij}\!=\!f_{ij}w_{ij}$ and by $W_{ii}\!=\!1\!-\!\sum_{n\neq m}\!W_{nm}$ for $i=j$. The acceptance probability
\begin{align*}
    w_{ij}=\min\left\{1,\frac{f_{ji}P(\bm{\sigma}_j)}{f_{ij}P(\bm{\sigma}_i)}\right\}
          =\min\left\{1,\frac{f_{ij}}{f_{ji}}\exp\left(-\frac{\mathcal{H}(\bm{\sigma}_j)-\mathcal{H}(\bm{\sigma}_i)}{kT}\right)\right\}
\end{align*}
then defines the Metropolis algorithm, which indeed
satisfies Condition~\eqref{align:con1},~\eqref{align:con2} and~\eqref{align:con3}.~\cite{Janke2012} 

The ergodicity condition depends on the exact shape of the microstate changing rule. For the numerical work the following two local update algorithms 
are implemented. 



\subsection*{Single spin update algorithm}

In this update method the transition $\bm{\sigma}_i$ to $\bm{\sigma}_j$ results from a single spin flip obviously satisfying Condition~\eqref{align:con4} 
because each possible state can be generated by a finite number of spin flips. Firstly, one chooses a spin at random corresponding
to $f_{ij}=1/N$. Secondly, this spin will be flipped with a probability $w_{ij}=\min\{1,\exp(-\Delta E/kT)\}$, where $\Delta E$ denotes the
energy deviation resulting from flipping the spin. The simulation steps are conventionally measured in sweeps, where one sweep contains $N$ spin flip 
proposals.

The advantage of this compact algorithm is that a huge number of systems can be simulated because a single spin flip can easily be applied to any "Ising like" 
system. However, for frustrated systems at low $T$ the proposals are almost certainly refused and a transition between the free energy minima requires 
a relatively huge number of accepted proposals, thus the system remains in the 
current microstate over a large number of spin flip proposals thereby expanding the simulation time to collect reliable data. Unluckily, the system under consideration 
is such a system, but the following update algorithm could be the solution to this dilemma. 



\subsection*{Line spin update algorithm}

In this update method the microstate transition will be enabled both through a line flip and a single spin flip. A line flip consists of flipping all 
the spins highlighted gray in Figure~\ref{fig:simulation1}. Of course, choosing other rows and columns is possible as well.
\begin{figure}[!h]
    \centering
        \input{images/lineflip.tex}
    \caption{This Figure shows two line flip proposal of the $2L$ possible ones.}
    \label{fig:simulation1}
\end{figure}
The additional single spin flip is necessary to satisfy Condition~\eqref{align:con4} because by only applying line flips, the entire state space 
cannot be covered. For example, the system is initiated in a nearly ideal ferromagnetic state, with one spin aligned in the opposite direction. 
There exists no sequence of line flips that transits the system from the initial state mentioned to an ideal ferromagnetic state, hence the system is 
trapped in a sub-state space. To apply this update method in practice, firstly choose a line at random of the $2L$ columns and rows with 
a selection probability $f_{ij}=1/2L$. This line will be flipped with a probability $w_{ij}=\min\{1,\exp(-\Delta E/kT)\}$, but now $\Delta E$ results 
from a flipped line. After the line flip proposal one has to perform $L$ spin flip proposals such that $L/2$ repetitions of this procedure correspond 
to one sweep. With this unit of simulation steps, the two depicted update algorithms are comparable. 

The advantage of the modified algorithm is to overcome the high rejection rate in the frustrating temperature range of the system. The line flips 
mentioned in Section~\ref{sec:GroundState} applied to an aligned ground-state cost no energy, therefore the line flips are accepted more frequently
at low $T$ because the system is in a near ground-state and a line flip has low energy costs. This line spin update 
algorithm was first presented in a paper of Kalz et al.~\cite{Kalz2008}




\section{Pseudo random number generators}

In the two previous subsection, the simulation algorithms depicted rely on stochastic experiments, in particular on the randomly
chosen lattice sites or lines and the acceptance of the flip proposals. Even for the smallest considered lattice length, the number 
of such stochastic experiments amounts to many millions. Rolling the dice is definitely not the way to go, performing these stochastic
experiments in a reasonable time. In computer simulation studies, pseudo random number generators (PRNG) overcome
this problem. PRNG are based on some deterministic rule, producing a sequence of numbers $X_i$ with a random face, i.e., $X_i$ appear 
to be drawn randomly. An example of such a deterministic rule is the linear congruential generators defined by the recursive sequence
\begin{align*}
    X_{i+1}=(aX_i+c)\mathrm{mod}(m),
\end{align*}
producing pseudo random numbers between $0$ and $m\!-\!1$, with $a,c,m\!\in\!\mathbb{N}$~\cite{Janke2002}. There are many of such algorithms,
with advantages and disadvantages for their purposes. The main criterions are the period length, i.e., the number of iteration 
until the same sequence returns, and the efficiency in context of the computational workload. Furthermore, some algorithms exhibit a "resonant" 
behavior with some application, i.e., the randomness fails, and unreliable data can be produced~\cite{Janke2002}. Therefore, care is 
necessary for the choice of the PRNG. 

For the numerical work in this thesis, the $32\mathrm{bit}$ Mersenne Twister \verb|mt19937| PRNG of 
the \verb|c++11| standard library is used, which is considered as very well tested and as a common choice for Monte Carlo simulation studies.
Additionally, this PRNG has a very large period length of $2^{19937}\!-\!1$. 

Many of the PRNG and particularly the Mersenne Twister generator produces uniformly distributed integers. An additional step is necessary to obtain 
non-uniform distributed pseudo random number. For explanation, let be $P(x)$ some invertible probability distribution of the random variable $X$ 
and $p\!\in\![0,1)$ a pseudo random number drawn uniformly. Thus, the probability of the event $X\!=\!x^*$ is given by $P(x^*)$. Then, a 
pseudo random number distributed according to $P(x)$ can be obtained through the inversion $x^*\!=\!P^{-1}(p)$.~\cite{Janke2002}




